The benchmarks we have chosen as a team are Stream, Linpack, Nuttcp, and MySQL.

The stream benchmark is basically “a simple synthetic benchmark program that measures sustainable memory bandwidth when performing simple operations”. Along with this, it also computes corresponding computation rates for simple vector kernels. It is also intended to measure the bandwidth from main memory. Stream is pretty much easy to run. However, there are many variations in OS’s and hardware, so it is difficult to comprehend. The Stream benchmark is run through C Language or through FORTRAN.

MySql is a relational database that is widely used in the cloud that typically stresses memory, IPC, filesystem, and networking subsystems. In the article, they ran the SysBench oltp benchmark against a single instance of MySql. SysBench provides measurements that are statistics of transaction latency and throughput in transactions/seconds. When the benchmark was ran, MySql was configured with a 3GB cache enabled, so it was sufficient enough to cache all reads during the benchmark runs.The oltp benchmark used a database that was preloaded with 2 million records and executed a fixed set of read/write transactions choosing between five SELECT queries, two UPDATE queries, a DELETE query and an INSERT. In this benchmark, five different configurations were measured. The first was that MySQL was running normally on Linux (native). Next, MySQL ran under Docker using host networking and a volume (Docker net=host volume). After, that it ran using a volume but normal Docker networking (Docker NAT volume). They tested that MySql ran while storing the database within the container filesystem (Docker NAT AUFS) and lastly, that MySQL ran under KVM. 
The results were as follows. Docker had similar performance to native, “with the difference asymptotically approaching 2% at higher concurrency”. KVM had much higher overhead, more than 40% in all measured cases. Different KVM storage protocols made no difference in performance for an in-cache workload. KVM showed saturation was achieved in the network, but not in the CPUs. It was also found that the benchmark generated a lot of small packets, so even though the network bandwidth was small, the network stack was not able to sustain the number of packets per second needed. Since the benchmark used synchronous requests, an increase in latency also reduced throughput at a given concurrency level. It was also found that the latency increased with load, but Docker increased the latency faster for moderate levels of load, which explained the lower throughput at low concurrency levels.

The benchmark also found that native Linux was able to achieve higher peak CPU utilization than Docker. The difference was around 1.5%. It was also found that the same concurrency with lower throughput for Docker, and Docker with NAT, did not create an equivalent increase in CPU consumption. The difference in throughput was minimal when the same amount of CPU was used. Due to mutex contention, they found that latency was not the same with Docker being substantially higher for lower values of concurrency. Mutex contention also prevented MySQL from fully utilizing the CPU in all cases, and was more pronounced in the Docker case due to longer transactions. In all, it was found that since Docker required more idle time, higher overhead did not impact throughput for the number of clients used in the benchmark.

Nuttcp is a network performance tool. It is intended for use by networks to measure the raw TCP or UDP network layer throughput. This happens by transferring memory buffers from a source system across an interconnecting network to a destination system. This data is transferred for a specific time interval or a specific number of bytes. nuttcp provides user info, system info, time info, cpu utilization (of the transmitter and receiver), and loss percentage. 
The nuttcp frame is based on nttcp which is an upgrade of the ttcp. Ttcp was originally to compare the performance of TCP stacks. Nuttcp has many upgrades that the aforementioned do not. Server mode, rate limitations, parallel streaming, and timers are some upgrades to list. IPv6 is now supported as well as IPv4 multicast.

